---
layout: default
title: Transformer on Graphs
permalink: /teaching/ws2024/tf_on_graph.md/
---
### **Seminar (Master): Transformers for Graphs**
Machine learning on graphs has seen a surge in interest due to the wide availability of graph data across a broad spectrum of disciplines, from life to social and engineering sciences. Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets. This seminar will discuss recent progress in graph transformers, focusing on a theoretical understanding.
<html lang="en">
	<body>
		<!-- <div id="wrapper"> -->
		<h4>Requirements for Passing</h4>
		<div style="padding: 15px; padding-bottom: 1px; {% if site.enable_darkmode %}background-color: #888;{% else %}background-color: #e0e5e0;{% endif %}">
		To pass the seminar, you need to fulfill the following:
		<br>
			<ol>
				<li>Give a 30-minute-long talk about your assigned paper.</li>
				<li>Write a 12- to 15-page (excluding title page) detailed report about your assigned paper.</li>
				<li>Peer-review your fellow students' reports.</li>
				<li>Attend all meetings and actively participate; see below for dates.</li>
			</ol>
		</div>
		<br>
		<h4>Talks</h4>
		At the end of the semester, each student will give a 30-minute-long talk about their assigned paper. You should provide an overview of your choosen/assigned paper and highlight the most important concepts and ideas. Ideally, your presentation should give the audience (i.e., your fellow students) a good understanding of your assigned paper.
		<br>
		<br>
		<h4>Reports</h4>
			The report gives a detailed overview of the assigned paper. The required report length is 12 to 15 pages, using the provided LaTeX <a href="./seminar_template.zip" download>template</a>. This means that after you get your paper assigned, you write your report and submit it for "peer review" by your fellow students. You will receive constructive feedback to improve the paper; afterward, you will receive additional feedback from the seminar organizers. You can then submit an updated, final version, which will be graded. Note that this means that you will also have to write some short reviews on the reports by your fellow students.
		<br>
		<br>
		<h4>Organization</h4>
			<ol>
				<li>More details are given during the mandatory kick-off meeting.</li>
				<li>Papers will be assigned after the kick-off meeting.</li>
				<li>The long talks will be presented in day-long block seminar.</li>
				<li><b>All meetings (kick-off and final talks) will take place in Room 228, Theaterstra√üe 35 - 39.</b></li>
			</ol>
		<br>
		<h4>Dates</h4>
			<table>
				<tr>
					<th align=left>Date</th>
					<th align=left></th>
				</tr>
				<tr>
					<td>11.10.2024, 12:00 &emsp;</td>
					<td>Kick-off-Meeting (in Person).</td>
				</tr>
				<tr>
					<td>04.11.2024, 24:00</td>
					<td>Submission of report drafts.</td>
				</tr>
				<tr>
					<td>02.12.2024, 24:00</td>
					<td>Submission of reports for peer review.</td>
				</tr>
				<tr>
					<td>11.12.2024, 24:00</td>
					<td>Submission of peer reviews..</td>
				</tr>		
				<tr>
					<td>13.12.2024, 12:00</td>
					<td>Discussion of peer reviews (in person).</td>
				</tr>
				<tr>
					<td>09.01.2025, 24:00</td>
					<td>Submission of reports.</td>
				</tr>
				<tr>
					<td>19.01.2025, 24:00</td>
					<td>Feedback by the organizers.</td>
				</tr>
				<tr>
					<td>31.01.2025, 24:00</td>
					<td>Submission of of presentation slides.</td>
				</tr>
				<tr>
					<td>04.02.2025, 12:00</td>
					<td>Peer review of presentation slides (in person).</td>
				</tr>
				<tr>
					<td>21.02.2025, 10:00</td>
					<td>Talks (in person).</td>
				</tr>
			</table>
		<br>
		<h4>Papers</h4>
		 The papers can be chosen from the following list.
			<ol>
				<li><a href="https://arxiv.org/abs/2404.03380">On the Theoretical Expressive Power and the Design Space of Higher-Order Graph Transformers
				<li><a href="https://arxiv.org/abs/2402.14202">Comparing Graph Transformers via Positional Encodings</a></li>
				<li><a href="https://arxiv.org/abs/2405.11951">Distinguished In Uniform: Self Attention Vs. Virtual Nodes</a></li>
				<li><a href="https://arxiv.org/abs/2301.11956">On the Connection Between MPNN and Graph Transformer</a></li>
				<li><a href="https://arxiv.org/abs/2310.02579">On the Stability of Expressive Positional Encodings for Graphs</a></li></a></li>
			</ol>
		<p></p>	
	</body>
</html>